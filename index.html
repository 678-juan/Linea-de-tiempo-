<!DOCTYPE html>
<html lang="es">
    <head>
        <meta charset="UTF-8">
        <title>
            Linea de tiempo
        </title>
        <link rel="stylesheet" href="css/styles.css">
    </head>

    <body>
        <h1>
            Linea de tiempo de la historia de la computacion
        </h1>

        <p>
            Selecciona un evento para ver su pagina individual con detalles, tabla y fuentes.
        </p>

        <!-- Lista principal de eventos con enlaces internos -->
        <h3>Eventos</h3>
        <ol>
            <li><a href="Antiguedad.html">3000 a.C. – Siglo IX | Antiguedad y Edad Media</a></li>
            <li><a href="era-mecanica.html">1642 – 1843 | Era Mecanica</a></li>
            <li><a href="era-electromecanica.html">1890 – 1941 | Era Electromecanica</a></li>
            <li><a href="primera-generacion.html">1940 | Primera Generacion</a></li>
            <li><a href="segunda-generacion.html">1956 | Segunda Generacion</a></li>
            <li><a href="tercera-generacion.html">1964 | Tercera Generacion</a></li>
            <li><a href="cuarta-generacion.html">1971 | Cuarta Generacion y Era Digital</a></li>
            <li><a href="microprocesador.html">1971 | Invencion del microprocesador</a></li>
            <li><a href="pc.html">1981 | Aparicion de la computadora personal (PC)</a></li>
            <li><a href="www.html">1990 | Creacion de la World Wide Web</a></li>
            <li><a href="smartphones.html">2007 | Computacion movil y smartphones</a></li>
            <li><a href="nube-bigdata.html">2010 | Computacion en la nube y Big Data</a></li>
            <li><a href="ia-2020.html">2020 | Inteligencia artificial y computacion avanzada</a></li>
        </ol>

        <!-- Tabla resumen para comparar periodos -->
        <h4>Resumen rapido</h4>
        <table>
            <tr>
                <th>Periodo</th>
                <th>Evento</th>
                <th>Enlace</th>
            </tr>
            <tr>
                <td>3000 a.C. – Siglo IX</td>
                <td>Abaco y algoritmos</td>
                <td><a href="Antiguedad.html">Ver</a></td>
            </tr>
            <tr>
                <td>1642 – 1843</td>
                <td>Maquinas de calculo</td>
                <td><a href="era-mecanica.html">Ver</a></td>
            </tr>
            <tr>
                <td>1890 – 1941</td>
                <td>Electricidad y tarjetas</td>
                <td><a href="era-electromecanica.html">Ver</a></td>
            </tr>
            <tr>
                <td>1940 – 1956</td>
                <td>Primera generacion</td>
                <td><a href="primera-generacion.html">Ver</a></td>
            </tr>
            <tr>
                <td>1956 – 1963</td>
                <td>Segunda generacion</td>
                <td><a href="segunda-generacion.html">Ver</a></td>
            </tr>
            <tr>
                <td>1964 – 1971</td>
                <td>Tercera generacion</td>
                <td><a href="tercera-generacion.html">Ver</a></td>
            </tr>
            <tr>
                <td>1971 – actualidad</td>
                <td>Cuarta generacion</td>
                <td><a href="cuarta-generacion.html">Ver</a></td>
            </tr>
        </table>

        <h2>3000 a.C. – Siglo IX | Antiguedad y Edad Media</h2>
        <p><em>Invención del ábaco y desarrollo de los algoritmos</em></p>
        <p>
            La historia de la computación inicia en la antigüedad, cuando las primeras civilizaciones necesitaron
            herramientas para contar, medir y administrar bienes. El ábaco, utilizado desde aproximadamente el año
            3000 a.C., permitió realizar operaciones matemáticas básicas como suma y resta, facilitando el comercio y
            la contabilidad. Este instrumento fue usado en diversas culturas como la china, la mesopotámica y la
            romana. Durante la Edad Media, el avance más significativo fue el aporte del matemático Al-Juarismi en el
            siglo IX, quien introdujo el concepto de algoritmo y promovió el uso del sistema numérico decimal. Estos
            aportes fueron esenciales para el desarrollo del pensamiento lógico y matemático, bases fundamentales de la
            computación moderna y de los lenguajes de programación actuales.
        </p>
        <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTGcSPE4I5aDb5rRHxlK0Dg9av_ZnO2ECnuSA&s" alt="Descripción de la imagen">
        <p><a href="Antiguedad.html">Ver pagina del evento</a></p>



        <hr>

        <h2>1642 – 1843 | Era Mecánica</h2>
        <p><em>Primeras máquinas de cálculo y origen de la programación</em></p>
        <p>
            La era mecánica marca el inicio de las máquinas diseñadas para realizar cálculos de forma automática. En
            1642, Blaise Pascal creó la Pascalina, una calculadora mecánica capaz de sumar y restar mediante engranajes.
            Posteriormente, en 1673, Gottfried Wilhelm Leibniz desarrolló una máquina que también podía multiplicar y
            dividir. En 1801, el telar de Jacquard introdujo el uso de tarjetas perforadas para controlar patrones
            textiles, concepto clave para la programación. El avance más importante ocurrió en 1837, cuando Charles
            Babbage diseñó la Máquina Analítica, que incluía memoria, unidad de control y procesamiento. Ada Lovelace
            escribió algoritmos para esta máquina, convirtiéndose en la primera programadora de la historia.
        </p>
        <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRQPQzRtBq2rh7cmpiTW5bmVG71hMw8pCh7XA&ss" alt="Descripción de la imagen">
        <p><a href="era-mecanica.html">Ver pagina del evento</a></p>
        <hr>

        <h2>1890 – 1941 | Era Electromecánica</h2>
        <p><em>Uso de electricidad y tarjetas perforadas</em></p>
        <p>
            La era electromecánica se caracteriza por la combinación de componentes mecánicos y eléctricos. En 1890,
            Herman Hollerith desarrolló una máquina tabuladora que utilizaba tarjetas perforadas para procesar datos del
            censo de Estados Unidos, reduciendo significativamente el tiempo de cálculo. Este invento dio origen a la
            empresa IBM. En 1936, Alan Turing presentó la Máquina de Turing, un modelo teórico que estableció los
            fundamentos de la computación y los algoritmos. En 1941, Konrad Zuse construyó la Z3, considerada la primera
            computadora programable automática. Estos avances permitieron pasar de simples cálculos mecánicos a sistemas
            capaces de ejecutar instrucciones complejas, sentando las bases para la computación electrónica moderna.
        </p>
        <img src="https://i.blogs.es/1a1233/tabuladora-02/650_1200.jpg" alt="Descripción de la imagen">    
        <p><a href="era-electromecanica.html">Ver pagina del evento</a></p>
        <hr>

        <h2>1940 – 1956 | Primera Generación</h2>
        <p><em>Computadoras con tubos al vacío</em></p>
        <p>
            La primera generación de computadoras se desarrolló entre 1940 y 1956 y utilizó tubos al vacío como
            principal componente electrónico. Estas máquinas eran extremadamente grandes, costosas y consumían grandes
            cantidades de energía. Un ejemplo representativo es el ENIAC, creado en 1946, capaz de realizar cálculos a
            una velocidad nunca antes vista. Sin embargo, las computadoras de esta generación eran poco confiables
            debido al sobrecalentamiento de los tubos. Además, se programaban exclusivamente en lenguaje máquina, lo que
            hacía su uso complejo. A pesar de sus limitaciones, estas computadoras fueron fundamentales para el
            desarrollo científico y militar, especialmente durante la Segunda Guerra Mundial, y marcaron el inicio de la
            computación electrónica.
        </p>
        <img src="https://concepto.de/wp-content/uploads/2019/07/generaciones-de-las-computadoras-e1563970798733.jpg" alt="Descripción de la imagen">      
        <p><a href="primera-generacion.html">Ver pagina del evento</a></p>
        <hr>

        <h2>1956 – 1964 | Segunda Generación</h2>
        <p><em>Uso de transistores y lenguajes de alto nivel</em></p>
        <p>
            La segunda generación de computadoras introdujo los transistores como reemplazo de los tubos al vacío. Esta
            mejora permitió crear computadoras más pequeñas, rápidas y confiables. Durante esta etapa surgieron los
            primeros lenguajes de programación de alto nivel, como FORTRAN y COBOL, lo que facilitó el desarrollo de
            software. Las computadoras comenzaron a utilizarse en empresas, universidades y entidades gubernamentales
            para tareas administrativas y científicas. Además, se mejoraron los sistemas de almacenamiento y
            procesamiento de datos. Esta generación representó un gran avance en la accesibilidad de la computación,
            permitiendo que más organizaciones pudieran beneficiarse de esta tecnología.
        </p>
        <img src="https://cdn.euroinnova.edu.es/img/subidasEditor/macintosh-g465970bc5_640%20(1)-1640321727.webp" alt="Descripción de la imagen">
        <p><a href="segunda-generacion.html">Ver pagina del evento</a></p>
            
        <hr>

        <h2>1964 – 1971 | Tercera Generación</h2>
        <p><em>Circuitos integrados y sistemas operativos</em></p>
        <p>
            La tercera generación se caracteriza por el uso de circuitos integrados, que permitieron integrar múltiples
            componentes electrónicos en un solo chip. Esto redujo el tamaño de las computadoras y aumentó su eficiencia
            y velocidad. Durante esta etapa surgieron los sistemas operativos, lo que permitió ejecutar varios programas
            simultáneamente. Las computadoras comenzaron a utilizar teclados y pantallas, mejorando la interacción con
            los usuarios. Su uso se expandió a industrias, centros de investigación y universidades. Esta generación fue
            clave para la evolución de la computación moderna, ya que hizo las máquinas más versátiles, potentes y
            fáciles de utilizar.
        </p>

        <img src="https://pdeinformacion.wordpress.com/wp-content/uploads/2015/02/tercera.jpg?w=432&h=288" alt="Descripción de la imagen">
        <p><a href="tercera-generacion.html">Ver pagina del evento</a></p>

        <hr>

        <h2>1971 – Actualidad | Cuarta Generación y Era Digital</h2>
        <p><em>Microprocesadores, Internet e inteligencia artificial</em></p>
        <p>
            La cuarta generación inició con la creación del microprocesador Intel 4004 en 1971, que permitió integrar
            la unidad central de procesamiento en un solo chip. Esto dio origen a las computadoras personales,
            facilitando su uso en hogares y oficinas. Con el tiempo surgieron Internet y la World Wide Web,
            revolucionando la comunicación y el acceso a la información. En la actualidad, la computación incluye
            tecnologías como la inteligencia artificial, la computación en la nube, los smartphones y el big data. Esta
            etapa continúa en constante evolución y tiene un impacto profundo en la educación, la economía y la vida
            cotidiana.
        </p>

        <img src="https://ulimndez.wordpress.com/wp-content/uploads/2011/04/ibmpcxt286-017.jpg?w=300" alt="Descripción de la imagen">
        <p><a href="cuarta-generacion.html">Ver pagina del evento</a></p>

        <hr>

        <h2>1971 | Invención del microprocesador</h2>
        <p><em>Nacimiento de la computadora personal</em></p>
        <p>
            En 1971, Intel presentó el microprocesador 4004, un avance revolucionario que permitió integrar la unidad
            central de procesamiento en un solo chip. Antes de este invento, las computadoras ocupaban grandes espacios
            y requerían múltiples componentes separados. El microprocesador hizo posible la creación de computadoras más
            pequeñas, económicas y accesibles. Este avance impulsó el desarrollo de las computadoras personales,
            permitiendo que la computación saliera de los laboratorios y grandes empresas para llegar a hogares, escuelas
            y oficinas. Además, facilitó la estandarización del hardware y el crecimiento de la industria del software.
            Sin el microprocesador, tecnologías actuales como laptops, tablets y teléfonos inteligentes no existirían.
            Este evento marca el inicio de la era moderna de la computación.
        </p>

        <img src="https://s1.significados.com/foto/microprocesador.jpg?class=article" alt="Descripción de la imagen">
        <p><a href="microprocesador.html">Ver pagina del evento</a></p>

        <hr>

        <h2>1981 | Aparición de la computadora personal (PC)</h2>
        <p><em>Popularización de la computación doméstica</em></p>
        <p>
            En 1981, IBM lanzó la IBM PC, consolidando el concepto de computadora personal. A diferencia de las
            computadoras anteriores, las PC estaban pensadas para usuarios comunes y no solo para expertos. Esto permitió
            que la informática se integrara a la vida cotidiana, especialmente en ámbitos como la educación, el trabajo
            administrativo y la gestión empresarial. Durante esta etapa surgieron empresas de software y sistemas
            operativos que facilitaron la interacción con las máquinas. Las computadoras personales impulsaron la
            alfabetización digital y cambiaron la forma de trabajar, estudiar y comunicarse. Este periodo sentó las bases
            del uso masivo de la tecnología informática en la sociedad moderna.
        </p>

        <img src="https://propart.es/wp-content/uploads/2018/12/IMG_20181205_120845-e1588426172357.jpg" alt="Descripción de la imagen">
        <p><a href="pc.html">Ver pagina del evento</a></p>

        <hr>

        <h2>1990 | Creación de la World Wide Web</h2>
        <p><em>Acceso global a la información</em></p>
        <p>
            En 1990, Tim Berners-Lee creó la World Wide Web, un sistema que permitió conectar documentos mediante
            enlaces y acceder a ellos a través de Internet. Este avance transformó radicalmente la comunicación y el
            acceso a la información. La web facilitó la creación de páginas, navegadores y servicios digitales,
            permitiendo que personas de todo el mundo compartieran conocimiento de forma rápida y sencilla. Gracias a la
            web, surgieron el comercio electrónico, las redes sociales, la educación virtual y los medios digitales. Este
            evento convirtió a Internet en una herramienta esencial para la vida moderna y marcó el inicio de la sociedad
            de la información.
        </p>

        <img src="https://upload.wikimedia.org/wikipedia/commons/d/d1/WWW-LetShare.svg" alt="Descripción de la imagen">
        <p><a href="www.html">Ver pagina del evento</a></p>

        <hr>

        <h2>2007 | Computación móvil y smartphones</h2>
        <p><em>Tecnología al alcance de la mano</em></p>
        <p>
            En 2007, la llegada de los smartphones marcó un antes y un después en la historia de la computación. Estos
            dispositivos integraron capacidades de cómputo avanzadas en un formato portátil, permitiendo acceso a
            Internet, aplicaciones, cámaras y sistemas de comunicación en tiempo real. La computación dejó de depender de
            un escritorio y pasó a acompañar a las personas en su vida diaria. Los smartphones impulsaron el desarrollo de
            aplicaciones móviles, redes sociales y servicios digitales, cambiando la forma de comunicarse, trabajar y
            entretenerse. Este avance consolidó la computación como una herramienta personal y permanente.
        </p>

        <img src="https://historythings.com/wp-content/uploads/2016/03/Mobile_phone_evolution-1.jpg" alt="Descripción de la imagen">
        <p><a href="smartphones.html">Ver pagina del evento</a></p>

        <hr>

        <h2>2010 | Computación en la nube y Big Data</h2>
        <p><em>Procesamiento y almacenamiento remoto</em></p>
        <p>
            A partir de 2010, la computación en la nube permitió almacenar y procesar información en servidores remotos
            accesibles desde cualquier lugar. Esto redujo la dependencia del hardware físico y facilitó el acceso a
            grandes volúmenes de datos. Paralelamente, el concepto de Big Data permitió analizar enormes cantidades de
            información para tomar mejores decisiones en áreas como la salud, la economía y la ciencia. Estas tecnologías
            transformaron la forma en que las empresas y los usuarios manejan la información, aumentando la eficiencia y
            la escalabilidad de los sistemas informáticos.
        </p>

        <img src="https://eadic.com/wp-content/uploads/2017/03/nube-1.jpg" alt="Descripción de la imagen">
        <p><a href="nube-bigdata.html">Ver pagina del evento</a></p>

        <hr>

        <h2>2020 – 2026 | Inteligencia artificial y computación avanzada</h2>
        <p><em>Máquinas que aprenden y toman decisiones</em></p>
        <p>
            Entre 2020 y 2026, la inteligencia artificial se convirtió en uno de los pilares de la computación moderna.
            Tecnologías como el aprendizaje automático, las redes neuronales y los asistentes virtuales permiten que las
            máquinas aprendan a partir de datos y realicen tareas complejas. Estas herramientas se utilizan en educación,
            medicina, videojuegos, seguridad y automatización industrial. Además, la computación cuántica comenzó a
            desarrollarse como una alternativa para resolver problemas imposibles para las computadoras tradicionales.
            Esta etapa representa una evolución en la que la computación no solo procesa información, sino que también
            analiza, predice y colabora activamente con los seres humanos.
        </p>
        <img src="https://insightlac.com/wp-content/uploads/2025/06/futuro-robot-inteligencia-artificial-fondo-sistema-red-1.jpg" alt="Descripción de la imagen">

        <p><a href="ia-2020.html">Ver pagina del evento</a></p>

        <h5>Notas</h5>
        <h6>Fin de la linea de tiempo</h6>
        
    </body>

</html>